'''
point discovery:
.predict函数需要更变为寻找yPre最大值
.大量数据样本时准确率疯狂下降原因：逆传播时，dw,db都要 /m
.模型改动：
    吴恩达深度学习的模型中，输出层 回传：[dZ = A - y]
    更改为：[dZ = (A - y) * g'(A)]    准确率提高0.03左右 （根据链式法则推导）
.留意dZ1 = np.dot(w2.T,dz2) * g'(z1)   和* g'(a1) 的区别 （吴恩达课程上是z1）
.增加ReLu函数和导数的实现，计算时间减少近一半

record:
.单隐层9单元：iter=3w,score=0.64

'''

# coding = 'utf-8'
import numpy as np
import matplotlib.image
import matplotlib.pyplot as plt
import time
from sklearn import datasets
from sklearn.datasets import load_digits # 手写字体data
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import train_test_split

np.set_printoptions(threshold=np.inf) # array全显示


class DeepNN:
    def __init__(self):
        self.layerNum = 0 # 总层数（包括输入输出层）
        self.n = [] # 每层的单元数（如 n[0] = 3 即第0层（输入层）单元数=3）
        self.W = [] # 每层权值 （如 w[1] = np.array([n[2],n[1])) ,用appand添加
        self.b = [] # 每层阈值 （如 b[1] = np.array([n1,1]) )
        self.learnRate = 0
        self.maxIter = 0


    def setModel(self,listOfN,maxIter,learnRate):
        # 设定层数和每层单元数,初始化w，b
        # paras：listOfN（一个List，如[3,2,3,4,1]即输入层3dim，第一个隐层单元数2....第三个隐层单元数4，输出层1dim
        self.layerNum = len(listOfN)
        self.n = listOfN
        self.maxIter = maxIter
        self.learnRate = learnRate

        # 初始化w,b
        self.W.append(0) # w0不存在，0占用位置
        self.b.append(0) # b0 不存在
        for i in range(1,self.layerNum):
            self.W.append(np.random.random([self.n[i],self.n[i-1]]) * 0.01)
            self.b.append(np.zeros([self.n[i],1]))
        '''w[4].shape = (n[4],n[3]) = (1,3) = 输出层'''
        #print(self.W[4])
        '''b[1].shape = (n[1],1) '''
        #print(self.b[1])


    def forwardP(self,trainX):
        # 前传
        # paras：trainX 传入训练集X，shape = （dim,m）
        # return：Z,A,m
        m = trainX.shape[1]
        A = list(range(self.layerNum)) # A[0]即第1层的Input，即trainX。A[1]即第1层的Output
        Z = list(range(self.layerNum)) # 用list存储np.array数据
        A[0] = trainX # A[0] = trainX
        for i in range(1,self.layerNum-1): # 注意此处for次数
            # 第1到n-1层（所有隐层）
            Z[i] = np.dot(self.W[i],A[i-1]) + self.b[i]
            A[i] = self.ReLuFunc(Z[i])
        # 输出层

        Z[self.layerNum-1] = np.dot(self.W[self.layerNum-1], A[self.layerNum-2]) + self.b[self.layerNum-1]
        A[self.layerNum-1] = self.sigmoidFunc(Z[self.layerNum-1])
        '''Z[3].shape = (3,m)  Z[4].shape = (1,m) = 输出 '''
        #print(Z[3].shape)

        return Z,A,m


    def backwardP(self,Z,A,m,trainY):
        # 逆传
        # paras:
        # return:
        dZ = list(range(self.layerNum)) # 0 -> 4    = 5
        dW = list(range(self.layerNum))
        db = list(range(self.layerNum))

        outN = self.layerNum - 1 # 输出层的序数（如总5层，输出层为4，输入层为0）
        # 输出层 回传
        dZ[outN] = (A[outN] - trainY) * self.sigmoidPrime(A[outN])
        dW[outN] = np.dot(dZ[outN], A[outN-1].transpose()) / m
        #print(A[outN-1].transpose().shape,"xxxxx")
        db[outN] = np.sum(dZ[outN], axis=1, keepdims=True) / m

        for i in range(outN-1,0,-1): # n-1 层到 1层（第0层不用算）
            # 隐层 回传
            dZ[i] = np.dot(self.W[i+1].transpose(),dZ[i+1]) * self.ReLuPrime(Z[i])
            # dot(W[4].T,dZ[4]) =>(3,1)和(1,m)=(3,m) 扩回上一层的shape
            dW[i] = np.dot(dZ[i],A[i-1].transpose())/m
            db[i] = np.sum(dZ[i],axis=1,keepdims=True)/m
        '''dW[1].shape = (n[1],trainX_dim) '''
        #print("xx",dW[1].shape)

        return dW,db


    def updateParas(self,dW,db):
        # 更新函数
        for i in range(self.layerNum):
            self.W[i] -= self.learnRate * dW[i]
            self.b[i] -= self.learnRate * db[i]


    def fit(self,trainX,trainY):
        # 训练函数
        # paras: trainX (shape= (dim,m)) , trainY (shape = (dim,m))
        #print("s1", self.W[2])
        for i in range(self.maxIter):
            Z,A,m = self.forwardP(trainX)
            dW,db = self.backwardP(Z,A,m,trainY)
            self.updateParas(dW,db)
        #print("s2",self.W[2])


    def predict(self,x):
        # predict
        # paras：x (shape = (dim,m)
        # return:yPredict （shape = (dim,m) )
        Z,A,m = self.forwardP(x)
        yPredict = A[self.layerNum - 1] # A[self.layerNum] 即输出层的A值 shape = (dim,m)

        # 找y[i]数列中最大一个变1，其余变0
        for i in range(x.shape[1]):
            for j in range(10):
                if yPredict[:,i].max() == yPredict[:,i][j]:
                    yPredict[j][i] = 1
                else:
                    yPredict[j][i] = 0
        return yPredict




    def score(self,testX,testY):
        # calc accuracy
        # paras: testX,testY    ( dim,m)
        # return: score
        m = testX.shape[1]
        coutR = 0. # 正确个数
        preY = self.predict(testX) # preY.shape = （dim,m)
        for i in range(m):
            for j in range(10):
                if preY[:,i][j] == 1 and testY[:,i][j] == 1:
                    coutR += 1

        return coutR/m


    def ReLuFunc(self,x):
        # ReLu 函数
        x = (np.abs(x) + x) /2.0

        return x


    def ReLuPrime(self,x):
        # ReLu 导数
        x[x <= 0] = 0
        x[x > 0] = 1

        return x

    def sigmoidFunc(self,x):
        # sigmoid函数
        # return 格式相同

        return 1.0 / (1 + np.exp(-x))

    def sigmoidPrime(self,x):
        # sigmoid 导数

        return x - np.power(x, 2)


def testHandWritingNum():
    #
    # load data
    digits = load_digits()

    # split
    trainX,testX,trainYraw,testYraw = train_test_split(digits.data,digits.target,test_size=0.25,random_state=33)

    # preprocess
    # trainX transpose to(dim,m)
    trainX = trainX.transpose()
    testX = testX.transpose()
    # trainY,testY  data preprocess(1dim -> 10dim)  to (dim,m)
    m1 = trainYraw.shape[0]
    m2 = testYraw.shape[0]
    trainY = np.zeros([10, m1])
    testY =np.zeros([10,m2])
    for i in range(m1):
        trainY[trainYraw[i]][i] = 1  # 例：Y = 2 -> Y = [0,0,1,0,0,0,0,0,0,0,0]
    for i in range(m2):
        testY[testYraw[i]][i] = 1

    # regular
    ss = StandardScaler()
    trainX = ss.fit_transform(trainX)
    testX = ss.fit_transform(testX)
    #print("反归一化", ss.inverse_transform(trainX[1]).astype(int))  # 反归一化

    # reduce dim


    # fit
    hwnNN = DeepNN()
    hwnNN.setModel([64,6,6,6,10],maxIter=50000,learnRate=0.03)
    hwnNN.fit(trainX,trainY)
    yPre = hwnNN.predict(testX)

    # predict & score

    print("label:",testYraw[:10],"\npredict",yPre[:,:10].transpose())
    print("score:",hwnNN.score(testX,testY))

    # show
    #plt.gray()
    #plt.matshow(digits.images[2])
    #plt.title(hwnNN.predict(digits.data[2].transpose()))
    #plt.show()


testHandWritingNum()
