#关键字：numpy.vstack()格式，numpy矩阵合并，更新公式在书上，w:= w - 2(X.transpose)(Xw - y)，矩阵对齐（行列维度）
#可以加入 对数几率回归算法（logistic regression），y = 1/(1+e^(Xw))

import numpy as np


def predict(x,weight,b):
    #预测模型，参数：训练x、权值weight(列向量）、截距b return：y值(列向量）
    #此模型被bGD函数调用，用于计算y值
    # model : y =  Xw
    yPre = np.dot(x, weight)
    return yPre


def bGD(x,y,weight,b,learnRate,trainN,maxIter):
    # 批量梯度下降，参数：训练集x（含ones（trainN）），y（列向量），weight权值组（含b），学习速度learnRate，样本数trainN(常数），最大迭代maxIter  retrun weight,b
    # bGD model ： w:= w - 2(X.transpose)(Xw - y)

    for i in range(0,maxIter+1):
        loss = predict(x,weight,b)-y
        if i < 10 or not(i%50):
            #print("迭代第", i, "\t次，weight值：", weight,"\tb值：",b)
            print("迭代第", i, "\t次，\nweight值：\n", weight[0:2],"\t b值：",weight[2])
        gradWeight = 2 * np.dot(x.transpose(),loss)
        #gradB = np.mean(predict(x,weight,b) - y)  #此处用b的误差 平均化作为b的梯度
        weight = weight - learnRate * gradWeight
        #b = b - learnRate * gradB
    return weight,b


learnRate = 0.007
maxIter = 100
b = 1 #阈值

# 训练数据x，y输入
trainData = np.array([[1,2],[2,3],[3,4],[4,5]],dtype=float)  #训练样本x 4组数据，2维
trainLabel = np.array([[1,3,4,5]],dtype=float).transpose()  #训练标记y，为便利加了transpose

# 转换矩阵，调整
trainN,trainDim = np.shape(trainData) #trainDim 数据维度（属性），trainN 数据数量（N组数据）
weight = np.array([[0.1],[0.1]])

trainData = np.hstack((trainData,np.ones([trainN,1]))) # 在X数组后面接上一个[[1],[1],....[1]] 列向量 补齐w和b的位置
weight = np.vstack([weight,b])  #注意vstack格式【】
#print(predict(trainData,weight,b))

# 执行梯度下降函数
weightbGD,bbGD = bGD(trainData,trainLabel,weight,b,learnRate,trainN,maxIter)

# 测试 部分
testData = np.array([[9,10],[11,12],[13,7],[8,9]],dtype=float) #测试样本
testData = np.hstack((testData,np.ones([testData.shape[0],1]))) # 在X数组后面接上一个[[1],[1],....[1]] 列向量 补齐w和b的位置
for i in range(testData.shape[0]):
    print("测试样本 x = ",testData[i][0:2],"预测结果：y = ",predict(testData,weightbGD,bbGD)[i])

# 画图
#这是立体的啊（二元函数）。。咋整不会画三维图
